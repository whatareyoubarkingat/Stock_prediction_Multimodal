# rag_engine_stock_1.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional, List
from datetime import datetime
import os

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import requests

# å¯é€‰å¯¼å…¥ streamlitï¼Œç”¨äºåœ¨ Cloud ä¸Šè¯»å– st.secrets
try:
    import streamlit as st  # type: ignore
except Exception:
    st = None  # type: ignore


# ============================================================
# åŸºæœ¬é…ç½®
# ============================================================

REQUIRED_COLS = ["date", "open", "high", "low", "close", "volume"]


# ============================================================
# æ–°é—»ç›¸å…³çš„æ•°æ®ç»“æ„ & å·¥å…·
# ============================================================

@dataclass
class NewsItem:
    title: str
    description: Optional[str]
    url: str
    source: str
    published_at: datetime
    content: Optional[str] = None


def _get_news_api_key() -> str:
    """
    å…ˆä» Streamlit Cloud çš„ secrets é‡Œæ‹¿ NEWS_API_KEYï¼Œ
    å¦‚æœæ²¡æœ‰ï¼Œå†é€€å›åˆ°ç¯å¢ƒå˜é‡ã€‚
    """
    # 1) å…ˆçœ‹ st.secrets
    if st is not None:
        for key in ("NEWS_API_KEY", "NEWSAPI_KEY", "NEWS_API_TOKEN"):
            try:
                if key in st.secrets:
                    return str(st.secrets[key])
            except Exception:
                # æœ¬åœ°æ²¡æœ‰ st.secrets ä¹‹ç±»çš„æƒ…å†µç›´æ¥å¿½ç•¥
                pass

    # 2) å†çœ‹ç¯å¢ƒå˜é‡
    for key in ("NEWS_API_KEY", "NEWSAPI_KEY", "NEWS_API_TOKEN"):
        val = os.getenv(key)
        if val:
            return val

    # 3) éƒ½æ²¡æœ‰å°±æŠ›é”™ï¼ˆå¤–å±‚ä¼š catchï¼Œä¸ä¼šè®©æ•´ä¸ª app å´©æ‰ï¼‰
    raise RuntimeError(
        "æœªæ‰¾åˆ° NewsAPI API Keyï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ– Streamlit secrets ä¸­è®¾ç½® "
        "NEWS_API_KEY / NEWSAPI_KEY / NEWS_API_TOKENã€‚"
    )


def search_stock_news(query: str, max_results: int = 10) -> List[NewsItem]:
    """
    ä½¿ç”¨ NewsAPI æœç´¢ä¸è‚¡ç¥¨ç›¸å…³çš„æœ€æ–°æ–°é—»ã€‚

    query: è‚¡ç¥¨ä»£ç æˆ–å…¬å¸åï¼Œå¦‚ "600519" / "è´µå·èŒ…å°" / "AAPL"
    max_results: è¿”å›çš„æœ€å¤§æ–°é—»æ•°
    """
    api_key = _get_news_api_key()
    url = "https://newsapi.org/v2/everything"

    params = {
        "q": query,
        "language": "zh",      # ä¸»è¦ä¸­æ–‡æ–°é—»ï¼›å¿…è¦æ—¶æ”¹æˆ "en" æˆ–å»æ‰
        "sortBy": "publishedAt",
        "pageSize": max_results,
        "apiKey": api_key,
    }

    resp = requests.get(url, params=params, timeout=15)
    resp.raise_for_status()
    data = resp.json()

    articles = data.get("articles", []) or []
    items: List[NewsItem] = []

    for art in articles:
        title = art.get("title") or ""
        if not title:
            continue

        description = art.get("description")
        url_ = art.get("url") or ""
        source_name = (art.get("source") or {}).get("name") or ""
        published_at_str = art.get("publishedAt") or ""
        try:
            published_at = datetime.fromisoformat(
                published_at_str.replace("Z", "+00:00")
            )
        except Exception:
            published_at = datetime.utcnow()

        content = art.get("content")
        items.append(
            NewsItem(
                title=title,
                description=description,
                url=url_,
                source=source_name,
                published_at=published_at,
                content=content,
            )
        )

    return items


# ============================================================
# K çº¿åŠ è½½ & ç‰¹å¾å·¥ç¨‹
# ============================================================

def load_ohlcv(csv_path: str) -> pd.DataFrame:
    """
    ä»æœ¬åœ° CSV åŠ è½½ OHLCVï¼Œç¡®ä¿æœ‰ï¼š
        date, open, high, low, close, volume
    ç°åœ¨ä¸»æµç¨‹ç”¨çš„æ˜¯ yfinanceï¼Œè¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ä¸ºäº†å…¼å®¹â€œä¸Šä¼  CSVâ€çš„è€é€»è¾‘ã€‚
    """
    df = pd.read_csv(csv_path)

    # å¤„ç† date
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"])
    else:
        # å¦‚æœæ²¡æœ‰ dateï¼Œå°±ç”¨ç¬¬ä¸€åˆ—å½“ä½œæ—¥æœŸ
        df.insert(0, "date", pd.to_datetime(df.iloc[:, 0]))

    # å…¼å®¹ yfinance é»˜è®¤åˆ—å
    rename_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume",
    }
    df = df.rename(columns=rename_map)

    # å¦‚æœæ²¡æœ‰ closeï¼Œå°±ç”¨ adj_close é¡¶ä¸Š
    if "close" not in df.columns and "adj_close" in df.columns:
        df["close"] = df["adj_close"]

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"CSV ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    df = df[REQUIRED_COLS].copy()
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").reset_index(drop=True)
    return df


def make_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¾“å…¥åŸå§‹ OHLCVï¼Œè¾“å‡ºåŒ…å«å„ç§æŠ€æœ¯æŒ‡æ ‡çš„ç‰¹å¾ DataFrameã€‚
    """
    if df is None or df.empty:
        raise ValueError("è¾“å…¥ df ä¸ºç©ºã€‚")

    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)

    x = df.copy()

    # ç»Ÿä¸€åˆ—å
    rename_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume",
    }
    x = x.rename(columns=rename_map)

    # æ—¥æœŸå¤„ç†
    if "date" in x.columns:
        x["date"] = pd.to_datetime(x["date"])
        x = x.sort_values("date").reset_index(drop=True)

    # å¿…è¦åˆ—æ£€æŸ¥
    missing = [c for c in REQUIRED_COLS if c not in x.columns]
    if missing:
        raise ValueError(f"make_features: ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    # â­â­ æ ¸å¿ƒä¿®å¤ï¼šé¿å…ä»»ä½•æƒ…å†µ fallback åˆ° pandas çš„ to_numeric â­â­
    num_cols = ["open", "high", "low", "close", "volume"]
    for c in num_cols:
        x[c] = x[c].astype(float)

    # æŠ€æœ¯æŒ‡æ ‡
    x["ret_1"] = x["close"].pct_change()

    for w in (3, 5, 10, 20):
        x[f"ma_{w}"] = x["close"].rolling(w).mean()
        x[f"ret_std_{w}"] = x["ret_1"].rolling(w).std()
        x[f"vol_ma_{w}"] = x["volume"].rolling(w).mean()

    # æ”¶ç›˜ä»·ç›¸å¯¹ MA20 çš„æ¯”å€¼
    ma20 = x["ma_20"].astype(float)
    close = x["close"].astype(float)

    # åŸå§‹æ¯”å€¼
    ratio = close / (ma20 + 1e-8)

    # å¼ºåˆ¶è½¬æˆ 1D
    ratio_1d = np.asarray(ratio, dtype=float).reshape(-1)

    # ğŸ”¥ å¼ºåˆ¶å¯¹é½é•¿åº¦ï¼šè¡¥é½ / æˆªæ–­ï¼Œè®©ç»“æœé•¿åº¦ä¸ x å®Œå…¨ä¸€è‡´
    if len(ratio_1d) != len(x):
        fixed = np.full(len(x), np.nan, dtype=float)
        L = min(len(ratio_1d), len(fixed))
        fixed[:L] = ratio_1d[:L]
        ratio_1d = fixed  # è¦†ç›–ä¸ºä¿®å¤åçš„ç‰ˆæœ¬

    # å†™å…¥åˆ—
    x["close_over_ma20"] = ratio_1d

    return x



# ============================================================
# éšæœºæ£®æ—æ¨¡å‹ï¼šä»…åŸºäºä»·æ ¼ç‰¹å¾
# ============================================================

@dataclass
class ForecastResult:
    forecast_df: pd.DataFrame
    test_mape: Optional[float]


class StockForecaster:
    """
    ä»…ä½¿ç”¨ä»·æ ¼ç‰¹å¾ï¼ˆmake_featuresï¼‰åšå›å½’é¢„æµ‹çš„ RandomForest å°è£…ã€‚
    """

    def __init__(
        self,
        n_estimators: int = 400,
        random_state: int = 42,
        min_train_size: int = 10,
    ):
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            random_state=random_state,
            n_jobs=-1,
        )
        self.min_train_size = min_train_size
        self.fitted = False

    # ------------------------------------------------------------

    def fit(self, df: pd.DataFrame) -> float:
        """
        ç”¨å†å²æ•°æ®è®­ç»ƒéšæœºæ£®æ—ï¼Œå¹¶è¿”å›ä¸€ä¸ªç®€å•çš„ Test MAPE ä½œä¸ºå‚è€ƒã€‚
        """
        feat = make_features(df).dropna().reset_index(drop=True)

        if len(feat) < self.min_train_size:
            # æ•°æ®å¤ªå°‘ï¼Œç›´æ¥å…¨é‡è®­ç»ƒï¼Œä¸åš train/test æ‹†åˆ†ï¼Œä¹Ÿä¸ç®— MAPE
            X = feat.drop(columns=["date", "close"]).values
            y = feat["close"].values
            self.model.fit(X, y)
            self.fitted = True
            return float("nan")


        X = feat.drop(columns=["date", "close"]).values
        y = feat["close"].values

        # ç®€å•åˆ’åˆ†ï¼šå‰ 80% è®­ç»ƒï¼Œå 20% æµ‹è¯•
        split_idx = int(len(feat) * 0.8)
        if split_idx <= 0 or split_idx >= len(feat):
            # æç«¯æƒ…å†µï¼šç›´æ¥å…¨é‡è®­ç»ƒï¼Œä¸ç®— MAPE
            self.model.fit(X, y)
            self.fitted = True
            return float("nan")

        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        self.model.fit(X_train, y_train)
        self.fitted = True

        y_pred = self.model.predict(X_test)
        eps = 1e-8
        mape = float(np.mean(np.abs((y_test - y_pred) / (y_test + eps))) * 100.0)
        return mape

    # ------------------------------------------------------------

    def predict_future(
        self,
        df: pd.DataFrame,
        horizon: int = 5,
    ) -> ForecastResult:
        """
        é€’å½’æ–¹å¼é¢„æµ‹æœªæ¥ horizon å¤©çš„ closeï¼Œå¹¶è¿”å› ForecastResultã€‚
        """
        if not self.fitted:
            mape = self.fit(df)
        else:
            mape = float("nan")

        # ç”¨ä¸€ä»½å¯å˜å‰¯æœ¬é€’æ¨æœªæ¥æ•°æ®
        hist = df.copy()
        if "date" not in hist.columns:
            raise ValueError("predict_future: df ä¸­ç¼ºå°‘ 'date' åˆ—ã€‚")

        hist["date"] = pd.to_datetime(hist["date"])
        hist = hist.sort_values("date").reset_index(drop=True)

        preds: List[float] = []
        dates: List[datetime] = []

        last_date = hist["date"].max()

        for _ in range(horizon):
            feat_all = make_features(hist).dropna().reset_index(drop=True)
            if feat_all.empty:
                raise RuntimeError("é¢„æµ‹æ—¶ç‰¹å¾ä¸ºç©ºï¼Œè¯·æ£€æŸ¥è¾“å…¥æ•°æ®ã€‚")

            last_row = feat_all.iloc[-1]
            X_last = last_row.drop(labels=["date", "close"]).values.reshape(1, -1)

            next_close = float(self.model.predict(X_last)[0])

            # è¿™é‡Œç®€å•åœ° +1 å¤©ï¼›å®é™…é¡¹ç›®å¯ä»¥æ”¹æˆâ€œåªåŠ äº¤æ˜“æ—¥â€çš„é€»è¾‘
            next_date = last_date + pd.Timedelta(days=1)
            last_date = next_date

            preds.append(next_close)
            dates.append(next_date)

            # ä¸ºäº†é€’å½’é¢„æµ‹ï¼Œæ„é€ â€œä¸‹ä¸€å¤©çš„ä¼ªæ•°æ®â€
            new_row = {
                "date": next_date,
                "open": next_close,
                "high": next_close,
                "low": next_close,
                "close": next_close,
                "volume": hist["volume"].iloc[-1],  # æ²¿ç”¨ä¸Šä¸€æ—¥æˆäº¤é‡
            }
            hist = pd.concat([hist, pd.DataFrame([new_row])], ignore_index=True)

        forecast_df = pd.DataFrame({"date": dates, "pred_close": preds})
        return ForecastResult(forecast_df=forecast_df, test_mape=mape)
