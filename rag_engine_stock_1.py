# rag_engine_stock_1.py
from __future__ import annotations
from dataclasses import dataclass
from typing import Optional, List
from datetime import datetime
import os

import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
import requests

# å¯é€‰å¯¼å…¥ streamlitï¼Œç”¨äºåœ¨ Cloud ä¸Šè¯»å– st.secrets
try:
    import streamlit as st  # type: ignore
except Exception:
    st = None  # type: ignore


# ============================================================
# åŸºæœ¬é…ç½®
# ============================================================

REQUIRED_COLS = ["date", "open", "high", "low", "close", "volume"]


# ============================================================
# æ–°é—»ç›¸å…³çš„æ•°æ®ç»“æ„ & å·¥å…·
# ============================================================

@dataclass
class NewsItem:
    title: str
    description: Optional[str]
    url: str
    source: str
    published_at: datetime
    content: Optional[str] = None


def _get_news_api_key() -> str:
    """
    å…ˆä» Streamlit Cloud çš„ secrets é‡Œæ‹¿ NEWS_API_KEYï¼Œ
    å¦‚æœæ²¡æœ‰ï¼Œå†é€€å›åˆ°ç¯å¢ƒå˜é‡ã€‚
    """
    # 1) å…ˆçœ‹ st.secrets
    if st is not None:
        for key in ("NEWS_API_KEY", "NEWSAPI_KEY", "NEWS_API_TOKEN"):
            try:
                if key in st.secrets:
                    return str(st.secrets[key])
            except Exception:
                # æœ¬åœ°æ²¡æœ‰ st.secrets ä¹‹ç±»çš„æƒ…å†µç›´æ¥å¿½ç•¥
                pass

    # 2) å†çœ‹ç¯å¢ƒå˜é‡
    for key in ("NEWS_API_KEY", "NEWSAPI_KEY", "NEWS_API_TOKEN"):
        val = os.getenv(key)
        if val:
            return val

    # 3) éƒ½æ²¡æœ‰å°±æŠ›é”™ï¼ˆå¤–å±‚ä¼š catchï¼Œä¸ä¼šè®©æ•´ä¸ª app å´©æ‰ï¼‰
    raise RuntimeError(
        "æœªæ‰¾åˆ° NewsAPI API Keyï¼Œè¯·åœ¨ç¯å¢ƒå˜é‡æˆ– Streamlit secrets ä¸­è®¾ç½® "
        "NEWS_API_KEY / NEWSAPI_KEY / NEWS_API_TOKENã€‚"
    )


def search_stock_news(query: str, max_results: int = 10) -> List[NewsItem]:
    """
    ä½¿ç”¨ NewsAPI æœç´¢ä¸è‚¡ç¥¨ç›¸å…³çš„æœ€æ–°æ–°é—»ã€‚

    query: è‚¡ç¥¨ä»£ç æˆ–å…¬å¸åï¼Œå¦‚ "600519" / "è´µå·èŒ…å°" / "AAPL"
    max_results: è¿”å›çš„æœ€å¤§æ–°é—»æ•°
    """
    api_key = _get_news_api_key()
    url = "https://newsapi.org/v2/everything"

    params = {
        "q": query,
        "language": "zh",      # ä¸»è¦ä¸­æ–‡æ–°é—»ï¼›å¿…è¦æ—¶æ”¹æˆ "en" æˆ–å»æ‰
        "sortBy": "publishedAt",
        "pageSize": max_results,
        "apiKey": api_key,
    }

    resp = requests.get(url, params=params, timeout=15)
    resp.raise_for_status()
    data = resp.json()

    articles = data.get("articles", []) or []
    items: List[NewsItem] = []

    for art in articles:
        title = art.get("title") or ""
        if not title:
            continue

        description = art.get("description")
        url_ = art.get("url") or ""
        source_name = (art.get("source") or {}).get("name") or ""
        published_at_str = art.get("publishedAt") or ""
        try:
            published_at = datetime.fromisoformat(
                published_at_str.replace("Z", "+00:00")
            )
        except Exception:
            published_at = datetime.utcnow()

        content = art.get("content")
        items.append(
            NewsItem(
                title=title,
                description=description,
                url=url_,
                source=source_name,
                published_at=published_at,
                content=content,
            )
        )

    return items


# ============================================================
# K çº¿åŠ è½½ & ç‰¹å¾å·¥ç¨‹
# ============================================================

def load_ohlcv(csv_path: str) -> pd.DataFrame:
    """
    ä»æœ¬åœ° CSV åŠ è½½ OHLCVï¼Œç¡®ä¿æœ‰ï¼š
        date, open, high, low, close, volume
    ç°åœ¨ä¸»æµç¨‹ç”¨çš„æ˜¯ yfinanceï¼Œè¿™ä¸ªå‡½æ•°ä¸»è¦æ˜¯ä¸ºäº†å…¼å®¹â€œä¸Šä¼  CSVâ€çš„è€é€»è¾‘ã€‚
    """
    df = pd.read_csv(csv_path)

    # å¤„ç† date
    if "date" in df.columns:
        df["date"] = pd.to_datetime(df["date"])
    else:
        # å¦‚æœæ²¡æœ‰ dateï¼Œå°±ç”¨ç¬¬ä¸€åˆ—å½“ä½œæ—¥æœŸ
        df.insert(0, "date", pd.to_datetime(df.iloc[:, 0]))

    # å…¼å®¹ yfinance é»˜è®¤åˆ—å
    rename_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume",
    }
    df = df.rename(columns=rename_map)

    # å¦‚æœæ²¡æœ‰ closeï¼Œå°±ç”¨ adj_close é¡¶ä¸Š
    if "close" not in df.columns and "adj_close" in df.columns:
        df["close"] = df["adj_close"]

    missing = [c for c in REQUIRED_COLS if c not in df.columns]
    if missing:
        raise ValueError(f"CSV ä¸­ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    df = df[REQUIRED_COLS].copy()
    df["date"] = pd.to_datetime(df["date"])
    df = df.sort_values("date").reset_index(drop=True)
    return df


def make_features(df: pd.DataFrame) -> pd.DataFrame:
    """
    è¾“å…¥åŸå§‹ OHLCVï¼Œè¾“å‡ºåŒ…å«å„ç§æŠ€æœ¯æŒ‡æ ‡çš„ç‰¹å¾ DataFrameã€‚
    """
    if df is None or df.empty:
        raise ValueError("è¾“å…¥ df ä¸ºç©ºã€‚")

    if not isinstance(df, pd.DataFrame):
        df = pd.DataFrame(df)

    x = df.copy()

    # ç»Ÿä¸€åˆ—å
    rename_map = {
        "Open": "open",
        "High": "high",
        "Low": "low",
        "Close": "close",
        "Adj Close": "adj_close",
        "Volume": "volume",
    }
    x = x.rename(columns=rename_map)

    # æ—¥æœŸå¤„ç†
    if "date" in x.columns:
        x["date"] = pd.to_datetime(x["date"])
        x = x.sort_values("date").reset_index(drop=True)

    # å¿…è¦åˆ—æ£€æŸ¥
    missing = [c for c in REQUIRED_COLS if c not in x.columns]
    if missing:
        raise ValueError(f"make_features: ç¼ºå°‘å¿…è¦åˆ—: {missing}")

    # â­â­ æ ¸å¿ƒä¿®å¤ï¼šé¿å…ä»»ä½•æƒ…å†µ fallback åˆ° pandas çš„ to_numeric â­â­
    num_cols = ["open", "high", "low", "close", "volume"]
    for c in num_cols:
        x[c] = x[c].astype(float)

    # æŠ€æœ¯æŒ‡æ ‡
    x["ret_1"] = x["close"].pct_change()

    for w in (3, 5, 10, 20):
        x[f"ma_{w}"] = x["close"].rolling(w).mean()
        x[f"ret_std_{w}"] = x["ret_1"].rolling(w).std()
        x[f"vol_ma_{w}"] = x["volume"].rolling(w).mean()

    # æ”¶ç›˜ä»·ç›¸å¯¹ MA20 çš„æ¯”å€¼
    ma20 = x["ma_20"].astype(float)
    close = x["close"].astype(float)

    # åŸå§‹æ¯”å€¼
    ratio = close / (ma20 + 1e-8)

    # å¼ºåˆ¶è½¬æˆ 1D
    ratio_1d = np.asarray(ratio, dtype=float).reshape(-1)

    # ğŸ”¥ å¼ºåˆ¶å¯¹é½é•¿åº¦ï¼šè¡¥é½ / æˆªæ–­ï¼Œè®©ç»“æœé•¿åº¦ä¸ x å®Œå…¨ä¸€è‡´
    if len(ratio_1d) != len(x):
        fixed = np.full(len(x), np.nan, dtype=float)
        L = min(len(ratio_1d), len(fixed))
        fixed[:L] = ratio_1d[:L]
        ratio_1d = fixed  # è¦†ç›–ä¸ºä¿®å¤åçš„ç‰ˆæœ¬

    # å†™å…¥åˆ—
    x["close_over_ma20"] = ratio_1d

    return x



# ============================================================
# éšæœºæ£®æ—æ¨¡å‹ï¼šä»…åŸºäºä»·æ ¼ç‰¹å¾
# ============================================================

@dataclass
class ForecastResult:
    forecast_df: pd.DataFrame
    test_mape: Optional[float]


class StockForecaster:
    """
    ä»…ä½¿ç”¨ä»·æ ¼ç‰¹å¾ï¼ˆmake_featuresï¼‰åšå›å½’é¢„æµ‹çš„ RandomForest å°è£…ã€‚
    ä¸å†å› ä¸ºâ€œæ•°æ®å¤ªå°‘â€æŠ›å¼‚å¸¸ï¼Œè€Œæ˜¯ï¼š
      - å®Œå…¨æ²¡æœ‰ç‰¹å¾è¡Œï¼šç›´æ¥è¿”å›ç©ºé¢„æµ‹ç»“æœ
      - ç‰¹å¾è¡Œæ•°å¾ˆå°‘ï¼šå…¨é‡è®­ç»ƒï¼Œä¸åš train/test æ‹†åˆ†ï¼ŒMAPE ç”¨ NaN
    """

    def __init__(
        self,
        n_estimators: int = 400,
        random_state: int = 42,
        min_train_size: int = 10,   # ä½ æƒ³è¦çš„é˜ˆå€¼
    ):
        self.model = RandomForestRegressor(
            n_estimators=n_estimators,
            random_state=random_state,
            n_jobs=-1,
        )
        self.min_train_size = min_train_size
        self.fitted = False

    # ------------------------------------------------------------
    def fit(self, df: pd.DataFrame) -> float:
        """
        ç”¨å†å²æ•°æ®è®­ç»ƒéšæœºæ£®æ—ï¼Œå¹¶è¿”å›ä¸€ä¸ªç®€å•çš„ Test MAPE ä½œä¸ºå‚è€ƒã€‚
        æ°¸è¿œä¸ä¸»åŠ¨ raise ValueErrorï¼ˆé¿å…æŠŠæ•´ä¸ª Streamlit app å¼„å´©ï¼‰ã€‚
        """
        feat = make_features(df).dropna().reset_index(drop=True)

        # 1) å®Œå…¨æ²¡æœ‰ç‰¹å¾è¡Œï¼šç›´æ¥æ”¾å¼ƒè®­ç»ƒï¼Œè¿”å› NaN
        if len(feat) == 0:
            # ä¸è®­ç»ƒæ¨¡å‹ï¼Œæ ‡è®°ä¸ºæœªè®­ç»ƒ
            self.fitted = False
            return float("nan")

        X_df = feat.drop(columns=["date", "close"])
        if X_df.shape[1] == 0:
            # æ²¡æœ‰ä»»ä½•å¯ç”¨ç‰¹å¾åˆ—ï¼Œä¹Ÿä¸è®­ç»ƒ
            self.fitted = False
            return float("nan")

        X = X_df.values
        y = feat["close"].values

        # 2) æ•°æ®å¤ªå°‘ï¼šå…¨é‡è®­ç»ƒï¼Œä¸æ‹† train/test
        if len(feat) < self.min_train_size:
            self.model.fit(X, y)
            self.fitted = True
            return float("nan")

        # 3) æ•°æ®è¶³å¤Ÿï¼šæ­£å¸¸åš 8:2 åˆ’åˆ†å¹¶è®¡ç®— MAPE
        split_idx = int(len(feat) * 0.8)
        if split_idx <= 0 or split_idx >= len(feat):
            # ç†è®ºä¸Šå¾ˆå°‘å‘ç”Ÿï¼Œå…œåº•æˆå…¨é‡è®­ç»ƒ
            self.model.fit(X, y)
            self.fitted = True
            return float("nan")

        X_train, X_test = X[:split_idx], X[split_idx:]
        y_train, y_test = y[:split_idx], y[split_idx:]

        self.model.fit(X_train, y_train)
        self.fitted = True

        y_pred = self.model.predict(X_test)
        eps = 1e-8
        mape = float(np.mean(np.abs((y_test - y_pred) / (y_test + eps))) * 100.0)
        return mape

    # ------------------------------------------------------------
    def predict_future(
        self,
        df: pd.DataFrame,
        horizon: int = 5,
    ) -> ForecastResult:
        """
        ä½¿ç”¨éšæœºæ£®æ— **åªé¢„æµ‹ä¸‹ä¸€å¤©çš„æ”¶ç›˜ä»·**ã€‚

        ä¸ºäº†å…¼å®¹å‰ç«¯çš„ horizon å‚æ•°ï¼š
        - å®é™…åªç”¨æ¨¡å‹é¢„æµ‹ T+1
        - ç„¶åæŠŠè¿™ä¸ª T+1 çš„é¢„æµ‹å€¼åœ¨æ—¶é—´è½´ä¸Šå¹³é“º horizon å¤©
          ï¼ˆè¿™æ ·å‰ç«¯è¿˜æ˜¯èƒ½ç”»å‡ºä¸€æ®µâ€œæœªæ¥æ›²çº¿â€ï¼Œä½†ä¸ä¼šå› ä¸ºé€’æ¨å‘æ•£å¾—ç¦»è°±ï¼‰
        """
        # 1. å¦‚æœè¿˜æ²¡è®­ç»ƒï¼Œå…ˆè®­ç»ƒä¸€æ¬¡
        if not self.fitted:
            mape = self.fit(df)
        else:
            mape = float("nan")

        # 2. åŸºæœ¬æ£€æŸ¥
        if "date" not in df.columns or "close" not in df.columns:
            raise ValueError("predict_future: df ä¸­ç¼ºå°‘ 'date' æˆ– 'close' åˆ—ã€‚")

        hist = df.copy()
        hist["date"] = pd.to_datetime(hist["date"])
        hist = hist.sort_values("date").reset_index(drop=True)

        # 3. ç”¨å…¨éƒ¨å†å²æ•°æ®åšä¸€æ¬¡ç‰¹å¾ï¼Œå–æœ€åä¸€è¡Œä½œä¸ºâ€œå½“å‰çŠ¶æ€â€
        feat_all = make_features(hist).dropna().reset_index(drop=True)
        if feat_all.empty:
            # æ²¡æœ‰ä»»ä½•ç‰¹å¾ï¼Œè¿”å›ç©ºç»“æœ
            return ForecastResult(
                forecast_df=pd.DataFrame(columns=["date", "pred_close"]),
                test_mape=mape,
            )

        last_row = feat_all.iloc[-1]
        X_last = last_row.drop(labels=["date", "close"]).values.reshape(1, -1)

        # 4. é¢„æµ‹ä¸‹ä¸€å¤©çš„ closeï¼ˆT+1ï¼‰
        next_close = float(self.model.predict(X_last)[0])

        # 5. æ„é€ æœªæ¥ horizon å¤©çš„æ—¥æœŸï¼Œå¹¶ç”¨åŒä¸€ä¸ªé¢„æµ‹å€¼å¹³é“º
        last_date = hist["date"].max()
        dates = []
        preds = []

        for i in range(1, horizon + 1):
            next_date = last_date + pd.Timedelta(days=i)
            dates.append(next_date)
            preds.append(next_close)

        forecast_df = pd.DataFrame({"date": dates, "pred_close": preds})
        return ForecastResult(forecast_df=forecast_df, test_mape=mape)

        # -------- æ­£å¸¸è®­ç»ƒæˆåŠŸçš„æƒ…å†µ --------
        hist = df.copy()
        hist["date"] = pd.to_datetime(hist["date"])
        hist = hist.sort_values("date").reset_index(drop=True)

        preds: List[float] = []
        dates: List[datetime] = []

        last_date = hist["date"].max()

        for _ in range(horizon):
            feat_all = make_features(hist).dropna().reset_index(drop=True)
            if feat_all.empty:
                break

            last_row = feat_all.iloc[-1]
            X_last = last_row.drop(labels=["date", "close"]).values.reshape(1, -1)

            next_close = float(self.model.predict(X_last)[0])

            next_date = last_date + pd.Timedelta(days=1)
            last_date = next_date

            preds.append(next_close)
            dates.append(next_date)

            new_row = {
                "date": next_date,
                "open": next_close,
                "high": next_close,
                "low": next_close,
                "close": next_close,
                "volume": hist["volume"].iloc[-1],
            }
            hist = pd.concat([hist, pd.DataFrame([new_row])], ignore_index=True)

        forecast_df = pd.DataFrame({"date": dates, "pred_close": preds})
        return ForecastResult(forecast_df=forecast_df, test_mape=mape)

